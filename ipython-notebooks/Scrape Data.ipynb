{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config params\n",
    "\n",
    "# Directory where all data will be stored\n",
    "DATA = Path('../data/')\n",
    "\n",
    "# Subreddits to get comments/posts from \n",
    "subreddits = ['Democrats', 'The_Donald', 'Politics', 'Conservative', 'Liberal']\n",
    "\n",
    "# Candidate comments\n",
    "candidates = {\n",
    "    'JoeBiden': ['joe', 'biden'],\n",
    "    'ElizabethWarren': ['elizabeth', 'warren'],\n",
    "    'BernieSanders': ['bernie', 'sanders'],\n",
    "    'DonaldTrump': ['donald', 'trump']\n",
    "}\n",
    "\n",
    "# File containing reddit app credentials \n",
    "CREDENTIALS = Path('../credentials.json')\n",
    "\n",
    "# Time period to get data for\n",
    "START = dt.datetime(year=2016, month=1, day=1)\n",
    "END = dt.datetime(year=2019, month=12, day=13, hour=23, minute=59, second=59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dirs(PATH, dirs):\n",
    "    \" Creates the required folders \"\n",
    "    \n",
    "    contents = [f.name for f in PATH.glob('*')]\n",
    "    \n",
    "    for directory in dirs:\n",
    "        if directory not in contents:\n",
    "            (PATH/f'{directory}').mkdir()\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "create_dirs(DATA, ['posts', 'comments', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up API access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushshift\n",
    "api = PushshiftAPI()\n",
    "\n",
    "# Auth Credentials\n",
    "creds = json.load(CREDENTIALS.open('rb'))\n",
    "\n",
    "# Authorize app\n",
    "reddit = praw.Reddit(**creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get all posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(FILELOC, subreddit, timeperiod, nchunks=1000,\n",
    "              filter_params = ['url','author', 'title', 'id', 'full_link', 'score', 'subreddit', \n",
    "                               'subreddit_subscribers', 'url'],\n",
    "              logging=True\n",
    "             ):\n",
    "    \"\"\" \n",
    "    Gets all the posts from a specific subredddit \n",
    "\n",
    "    Args:\n",
    "        FILELOC: Pathlib object specifying full path to file (including name and format)\n",
    "        subreddit: String specifying the name of the subreddit to get data from (e.g. learnpython).\n",
    "        timeperiod: Tuple of two datetime objects specifying the time period for data curation\n",
    "        nchunks: Number of chunks to divide the timeperiod in (default is good).\n",
    "        filter_params: List containing params to return for each post.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas Dataframe containing all the scraped posts\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start, end = [t.timestamp() for t in [START, END]]\n",
    "    difference = end - start\n",
    "    increment = difference/nchunks\n",
    "    assert difference > 0, \"Please enter the timeperiod in the correct format\"\n",
    "\n",
    "    with open(FILELOC, 'w') as f:\n",
    "        for i in range(0, nchunks):\n",
    "            if logging:\n",
    "                print(f\"Getting chunk: {i+1}/{nchunks}\")\n",
    "            \n",
    "            for p in api.search_submissions(after=start + i*difference, \n",
    "                                            before=start + (i+1)*difference,\n",
    "                                            subreddit=subreddit,\n",
    "                                            filter=filter_params,\n",
    "                                            limit=1e6):\n",
    "                f.write(json.dumps(p.d_) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dirs(DATA/'posts', ['raw', 'processed'])\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    print(f\"Getting posts for subreddit: {subreddit}\")\n",
    "    get_posts(DATA/f'posts/raw/{subreddit}.json', 'learnpython', (START,END))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get comments mentioning candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(FILELOC, query_term, subreddits, timeperiod, nchunks=1000,\n",
    "                 filter_params = ['url', 'author', 'body', 'id', 'full_link', 'score', 'subreddit'],\n",
    "                 logging=True):\n",
    "    \"\"\" \n",
    "    Gets all comments containing a specific term from the subreddits specified.\n",
    "\n",
    "    Args:\n",
    "        FILELOC: Pathlib object specifying full path to file (including name and format)\n",
    "        subreddits: List containing the names of the subreddits from which comments will be scraped.\n",
    "        timeperiod: Tuple of two datetime objects specifying the time period for data curation\n",
    "        nchunks: Number of chunks to divide the timeperiod in (default is good).\n",
    "        filter_params: List containing params to return for each post.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas Dataframe containing all the scraped posts\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start, end = [t.timestamp() for t in [START, END]]\n",
    "    difference = end - start\n",
    "    increment = difference/nchunks\n",
    "    assert difference > 0, \"Please enter the timeperiod in the correct format\"\n",
    "\n",
    "    with open(FILELOC, 'w') as f:\n",
    "        for i in range(0, nchunks):\n",
    "            if logging:\n",
    "                print(f\"Getting chunk: {i+1}/{nchunks}\")\n",
    "            \n",
    "            for subreddit in subreddits:\n",
    "                for p in api.search_comments(q=query_term,\n",
    "                                             after=start + i*difference, \n",
    "                                             before=start + (i+1)*difference,\n",
    "                                             subreddit=subreddit,\n",
    "                                             filter=filter_params,\n",
    "                                             limit=1e6):\n",
    "                    allcomments.append(p.d_)\n",
    "                    f.write(json.dumps(p.d_) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dirs(DATA/'comments', ['raw', 'processed'])\n",
    "\n",
    "for candidate, candidate_terms in candidates.items():\n",
    "    print(f\"Getting comments for candidate: {candidate}\")\n",
    "    \n",
    "    for term in candidate_terms:\n",
    "        get_comments(DATA/f'comments/raw/{term}.json', term, subreddits, (START,END))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

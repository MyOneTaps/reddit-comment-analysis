{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases, ldamodel\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config params\n",
    "\n",
    "# Data directory\n",
    "DATA = Path('../../reddit/data/')\n",
    "\n",
    "# Define stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Set random state for topic model experiments\n",
    "seed = 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read consolidated and filtered posts file\n",
    "allposts = pd.read_csv(DATA/'posts/processed/filtered_posts.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, remove_stopwords=True):\n",
    "    if remove_stopwords:\n",
    "        return [s for s in simple_preprocess(text) if s not in stop_words]\n",
    "    \n",
    "    else:\n",
    "        simple_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 8s, sys: 1.09 s, total: 2min 9s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Tokenize post titles\n",
    "allposts['tokenized_titles'] = allposts['title'].apply(tokenize)\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(allposts['tokenized_titles'])\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in allposts['tokenized_titles']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5d 5h 34min 8s, sys: 6h 53min 58s, total: 5d 12h 28min 6s\n",
      "Wall time: 22h 15min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lda_model = ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=30, \n",
    "    random_state=seed,\n",
    "    update_every=1,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24,\n",
      "  '0.000*\"babb\" + 0.000*\"pedofag\" + 0.000*\"drivethemout\" + '\n",
      "  '0.000*\"iwillredpillyou\" + 0.000*\"lanier\" + 0.000*\"kippy\" + 0.000*\"hiphop\" + '\n",
      "  '0.000*\"geographically\" + 0.000*\"abdominal\" + 0.000*\"stansted\"'),\n",
      " (13,\n",
      "  '0.401*\"children\" + 0.206*\"legal\" + 0.102*\"future\" + 0.073*\"faces\" + '\n",
      "  '0.039*\"professor\" + 0.028*\"consequences\" + 0.000*\"geographically\" + '\n",
      "  '0.000*\"pedofag\" + 0.000*\"manchesterian\" + 0.000*\"mobb\"'),\n",
      " (22,\n",
      "  '0.178*\"security\" + 0.137*\"social\" + 0.125*\"questions\" + 0.118*\"senator\" + '\n",
      "  '0.114*\"confirmed\" + 0.062*\"half\" + 0.059*\"pm\" + 0.045*\"scheme\" + '\n",
      "  '0.026*\"raped\" + 0.013*\"rick\"'),\n",
      " (18,\n",
      "  '0.228*\"thought\" + 0.188*\"emails\" + 0.130*\"mass\" + 0.108*\"nobody\" + '\n",
      "  '0.103*\"muslim\" + 0.074*\"murder\" + 0.038*\"cares\" + 0.000*\"biden\" + '\n",
      "  '0.000*\"geographically\" + 0.000*\"pedofag\"'),\n",
      " (7,\n",
      "  '0.190*\"next\" + 0.180*\"paul\" + 0.102*\"rand\" + 0.093*\"debate\" + '\n",
      "  '0.076*\"facebook\" + 0.060*\"ceo\" + 0.057*\"posted\" + 0.047*\"stage\" + '\n",
      "  '0.039*\"ben\" + 0.027*\"outraged\"'),\n",
      " (3,\n",
      "  '0.368*\"say\" + 0.172*\"top\" + 0.159*\"tax\" + 0.084*\"close\" + 0.038*\"double\" + '\n",
      "  '0.037*\"releases\" + 0.031*\"income\" + 0.015*\"meetings\" + 0.011*\"capital\" + '\n",
      "  '0.000*\"biden\"'),\n",
      " (6,\n",
      "  '0.508*\"says\" + 0.318*\"campaign\" + 0.037*\"program\" + 0.031*\"starts\" + '\n",
      "  '0.031*\"definitely\" + 0.000*\"impeachment\" + 0.000*\"ukraine\" + 0.000*\"biden\" '\n",
      "  '+ 0.000*\"like\" + 0.000*\"geographically\"'),\n",
      " (15,\n",
      "  '0.155*\"give\" + 0.131*\"live\" + 0.093*\"leader\" + 0.074*\"due\" + 0.054*\"rate\" + '\n",
      "  '0.046*\"rule\" + 0.042*\"radical\" + 0.040*\"demands\" + 0.039*\"students\" + '\n",
      "  '0.038*\"hits\"'),\n",
      " (9,\n",
      "  '0.208*\"pro\" + 0.176*\"bill\" + 0.174*\"woman\" + 0.146*\"life\" + 0.092*\"fired\" + '\n",
      "  '0.090*\"rights\" + 0.034*\"constitutional\" + 0.008*\"introduces\" + '\n",
      "  '0.007*\"oregon\" + 0.000*\"ukraine\"'),\n",
      " (4,\n",
      "  '0.368*\"said\" + 0.144*\"racist\" + 0.132*\"bernie\" + 0.074*\"sanders\" + '\n",
      "  '0.069*\"ice\" + 0.057*\"straight\" + 0.045*\"minutes\" + 0.021*\"actor\" + '\n",
      "  '0.020*\"version\" + 0.000*\"ukraine\"'),\n",
      " (29,\n",
      "  '0.246*\"going\" + 0.120*\"way\" + 0.114*\"best\" + 0.087*\"anyone\" + 0.079*\"days\" '\n",
      "  '+ 0.078*\"saying\" + 0.029*\"taxes\" + 0.028*\"health\" + 0.028*\"watching\" + '\n",
      "  '0.023*\"visit\"'),\n",
      " (8,\n",
      "  '0.169*\"world\" + 0.139*\"party\" + 0.123*\"shit\" + 0.121*\"must\" + '\n",
      "  '0.096*\"americans\" + 0.083*\"general\" + 0.055*\"sex\" + 0.041*\"group\" + '\n",
      "  '0.039*\"ban\" + 0.027*\"attacks\"'),\n",
      " (21,\n",
      "  '0.314*\"media\" + 0.223*\"man\" + 0.117*\"two\" + 0.095*\"liberal\" + 0.053*\"guns\" '\n",
      "  '+ 0.052*\"admits\" + 0.025*\"paris\" + 0.023*\"per\" + 0.016*\"eu\" + '\n",
      "  '0.014*\"refugees\"'),\n",
      " (10,\n",
      "  '0.345*\"democrats\" + 0.189*\"america\" + 0.109*\"report\" + 0.071*\"calls\" + '\n",
      "  '0.070*\"run\" + 0.046*\"week\" + 0.037*\"lied\" + 0.021*\"cities\" + '\n",
      "  '0.017*\"activist\" + 0.016*\"personal\"'),\n",
      " (0,\n",
      "  '0.165*\"today\" + 0.153*\"think\" + 0.132*\"need\" + 0.091*\"police\" + '\n",
      "  '0.073*\"wall\" + 0.070*\"actually\" + 0.049*\"paid\" + 0.044*\"second\" + '\n",
      "  '0.041*\"amendment\" + 0.038*\"ready\"'),\n",
      " (11,\n",
      "  '0.323*\"new\" + 0.124*\"still\" + 0.091*\"government\" + 0.089*\"may\" + '\n",
      "  '0.088*\"support\" + 0.073*\"end\" + 0.048*\"elected\" + 0.031*\"college\" + '\n",
      "  '0.029*\"leave\" + 0.022*\"rape\"'),\n",
      " (25,\n",
      "  '0.321*\"one\" + 0.176*\"american\" + 0.109*\"video\" + 0.084*\"gun\" + 0.060*\"isis\" '\n",
      "  '+ 0.053*\"liberals\" + 0.043*\"kill\" + 0.034*\"game\" + 0.025*\"favorite\" + '\n",
      "  '0.023*\"involved\"'),\n",
      " (27,\n",
      "  '0.174*\"right\" + 0.090*\"also\" + 0.077*\"change\" + 0.073*\"climate\" + '\n",
      "  '0.072*\"free\" + 0.072*\"called\" + 0.065*\"hate\" + 0.045*\"speech\" + '\n",
      "  '0.042*\"shows\" + 0.031*\"reporter\"'),\n",
      " (1,\n",
      "  '0.427*\"president\" + 0.192*\"obama\" + 0.140*\"would\" + 0.117*\"vote\" + '\n",
      "  '0.054*\"court\" + 0.015*\"anymore\" + 0.015*\"supreme\" + 0.013*\"needed\" + '\n",
      "  '0.006*\"consider\" + 0.000*\"biden\"'),\n",
      " (16,\n",
      "  '0.849*\"trump\" + 0.075*\"donald\" + 0.016*\"bring\" + 0.013*\"conservatives\" + '\n",
      "  '0.011*\"welcome\" + 0.009*\"warning\" + 0.005*\"uses\" + 0.003*\"sarah\" + '\n",
      "  '0.002*\"endorses\" + 0.000*\"whistleblower\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 192 ms, sys: 0 ns, total: 192 ms\n",
      "Wall time: 194 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lda_model.save(fname=str(DATA/'lda_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -33.83386822880872\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CoherenceModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-eacbdc545be3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Compute Coherence Score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcoherence_model_lda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_lemmatized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c_v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcoherence_lda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoherence_model_lda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nCoherence Score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence_lda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CoherenceModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the bigram and trigram models\n",
    "# bigram = Phrases(tokenized_sents, min_count=20, \n",
    "#                  threshold=100) \n",
    "\n",
    "# trigram = gensim.models.Phrases(bigram[data_words],\n",
    "#                                 threshold=100)  \n",
    "\n",
    "# # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "# bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# # See trigram example\n",
    "# print(trigram_mod[bigram_mod[data_words[0]]])\n",
    "\n",
    "# def make_bigrams(texts):\n",
    "#     return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "# def make_trigrams(texts):\n",
    "#     return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "#\n",
    "# def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "#     \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "#     texts_out = []\n",
    "#     for sent in texts:\n",
    "#         doc = nlp(\" \".join(sent)) \n",
    "#         texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "#     return texts_out\n",
    "#\n",
    "# # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# # python3 -m spacy download en\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# # Do lemmatization keeping only noun, adj, vb, adv\n",
    "# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# print(data_lemmatized[:1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
